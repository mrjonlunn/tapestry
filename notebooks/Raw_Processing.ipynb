{"cells":[{"cell_type":"markdown","source":["# Metadata driven Extract, Transform And Load process\n","#### Project Tapestry - https://github.com/mrjonlunn/tapestry\n","##### Created by: Jon Lunn\n","##### Version 0.2"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a1ae0a61-ac3e-461e-afee-c17399089bee"},{"cell_type":"code","source":["# Add your own parameters here, and call it from a pipeline if you like\n","filePath = 'Files/Landing/testsource/testentity/NYC1000.csv'\n","configFolder = '/lakehouse/default/Files/PipelineMetaData/config/'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"1aa4aa50-daf9-4c4c-8027-baa2fea352dc"},{"cell_type":"code","source":["%run Standard_NoteBooks"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"391823a5-18a1-44c6-abe9-92d2879f9730"},{"cell_type":"code","source":["# Gets the config metadata\n","config = metadataLoader(filePath, configFolder)\n","\n","# Sets a load of config variables for later use\n","\n","#Standard options\n","configFileType = config[\"fileType\"] #string\n","configSource = config[\"dataset\"][\"source\"] #string\n","configEntity = config[\"dataset\"][\"entity\"] #string\n","customSchema = jsonToSchema(config[\"fileOptions\"][\"customSchema\"]) #struct\n","\n","# File options\n","if config[\"fileType\"] == 'csv':\n","    header = config[\"fileOptions\"][\"header\"] #string\n","    delimiter = config[\"fileOptions\"][\"delimiter\"] #string\n","    multiLine = config[\"fileOptions\"][\"multiLine\"] #string\n","    escape = config[\"fileOptions\"][\"escape\"] #string\n","    \n","elif fileType == 'json':\n","    print('not ready yet')\n","\n","# Table options\n","# Raw layer\n","rawLakehouseName = config[\"tableOptions\"][\"raw\"][\"lakehouseName\"] #string\n","rawLayerName = config[\"tableOptions\"][\"raw\"][\"layerName\"] #string\n","rawTableName = config[\"tableOptions\"][\"raw\"][\"tableName\"] #string\n","rawInsertType = config[\"tableOptions\"][\"raw\"][\"insertType\"] #string\n","rawCreateTableIfNotExists = config[\"tableOptions\"][\"raw\"][\"createTableIfNotExists\"] #boolean\n","rawPartitionType = config[\"tableOptions\"][\"raw\"][\"partitionType\"] #string\n","rawPartitionDateFormat = config[\"tableOptions\"][\"raw\"][\"partitionDateFormat\"] #string\n","rawDataframePartitionColumns = config[\"tableOptions\"][\"raw\"][\"dataframePartitionColumns\"] #List\n","rawTablePartitionColumns = config[\"tableOptions\"][\"raw\"][\"tablePartitionColumns\"] #List\n","rawPartitionRowSize = config[\"tableOptions\"][\"raw\"][\"partitionRowSize\"] #string\n","rawMergeOnColumns = config[\"tableOptions\"][\"raw\"][\"mergeOnColumns\"] #List\n","rawMergeUpdateColumns = config[\"tableOptions\"][\"raw\"][\"mergeUpdateColumns\"] #List"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a85d615e-cc9c-4ae7-9192-91fa02faf40a"},{"cell_type":"code","source":["# Loaders based on the file type\n","# Loader interfaces in loaders note book\n","\n","if configFileType == 'csv':\n","    df = loadCSV(filePath, header, delimiter, multiLine, customSchema, escape)\n","\n","elif configFileType == 'json':\n","    #df = loadJson()\n","    print('not done yet')\n","elif configFileType == 'xml':\n","    #df = loadJson()\n","    print('not done yet')\n","elif configFileType == 'parquet':\n","    #df = loadJson()\n","    print('not done yet')\n","else:\n","    raise Exception('File extension not recognised, or the extension of the file is not the same as the file type defined in the config file')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3ac21ec0-2b66-4bf5-8a87-b34dfbed28ca"},{"cell_type":"code","source":["# Process the dataframe, and get it ready for writing to a table\n","\n","# Extend the dataframe with the columns to partition on\n","if rawPartitionType.lower() == 'date':\n","    df = createDatePartitions(df, rawPartitionDateFormat, rawDataframePartitionColumns)\n","# elif rawPartitionType.lower() == 'reference':\n","#     df = createReferencePartitions(df, rawPartitionDateFormat, rawDataframePartitionColumns)\n","# elif rawPartitionType.lower() == 'businessKeys':\n","#     df = createBusinessKeysPartitions(df, rawPartitionDateFormat, rawDataframePartitionColumns)\n","\n","# Get dataframe schema\n","\n","writeSchema = df.schema\n","# Does the table exist?\n","# Notes: In checking the schema we could do schema merge, however that should be handled by a version. \n","# If you are happy using schema merge into Raw remove the raise exception, \n","# and create a merge schema flag in the JSON confiig and change the write process to the table for append and merge\n","if spark.catalog.tableExists(rawTableName, rawLakehouseName) == False:\n","    createTableIfNotExists(rawLakehouseName, rawLayerName, rawTableName, writeSchema, rawTablePartitionColumns, rawPartitionType)\n","elif spark.catalog.tableExists(rawTableName, rawLakehouseName) == True:\n","    if schemaDFToTable(rawLakehouseName, rawTableName, writeSchema) == False:\n","        raise Exception('There is a schema mis-match between the dataframe and the table')\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3b54e389-f631-42ab-8fec-1a7097f6b861"},{"cell_type":"code","source":["if rawLayerName != \"\":\n","    adjustedTableName = rawLakehouseName + '.' +rawLayerName + '_' + rawTableName\n","else:\n","    adjustedTableName = rawTableName\n","\n","try:\n","    if rawInsertType.lower() == 'append':\n","        df.write.format(\"delta\").mode(\"append\").saveAsTable(adjustedTableName)\n","        \n","    elif rawInsertType.lower() == 'overwrite':\n","        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(adjustedTableName)\n","\n","    elif rawInsertType.lower() == 'merge':\n","\n","        mergeOnColumns = \" AND \".join([f\"target.{col} = source.{col}\" for col in rawMergeOnColumns])\n","        updateColumns = {f\"target.{col}\": f\"source.{col}\" for col in rawMergeUpdateColumns}\n","\n","        deltaTable.alias(\"target\").merge(\n","            source=df.alias(\"source\"), \n","            condition = mergeOnColumns\n","        ).whenMatchedUpdate(\n","            set = updateColumns\n","        ).whenNotMatchedInsertAll().execute()\n","\n","    moveLandingFile(filePath, 'Success')\n","except:\n","    moveLandingFile(filePath, 'Failure')   \n","    \n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e3f7a20c-34cc-491d-b359-f708354ec5dc"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}],"default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896"}}},"nbformat":4,"nbformat_minor":5}