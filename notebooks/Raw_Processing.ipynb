{"cells":[{"cell_type":"markdown","id":"a1ae0a61-ac3e-461e-afee-c17399089bee","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Metadata driven Extract, Transform And Load process\n","#### Project Tapestry - https://github.com/mrjonlunn/tapestry\n","##### Created by: Jon Lunn\n","##### Version 0.3"]},{"cell_type":"code","execution_count":null,"id":"1aa4aa50-daf9-4c4c-8027-baa2fea352dc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["# Add your own parameters here, and call it from a pipeline if you like\n","file_path = 'Files/Landing/testsource/testentity/NYC1000.csv'\n","data_source = 'testSource'\n","entity = 'testEntity'"]},{"cell_type":"code","execution_count":null,"id":"391823a5-18a1-44c6-abe9-92d2879f9730","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%run Standard_NoteBooks"]},{"cell_type":"code","execution_count":null,"id":"33227691","metadata":{},"outputs":[],"source":["# Gets the config metadata\n","config = metadata_loader(data_source, entity)\n","\n","# Function to create variables from the config file structure\n","def create_variables(config):\n","    # Puts config_ in front of the structure items\n","    prefix = 'config_'\n","    for key, value in config.items():\n","        if isinstance(value, dict()):\n","            create_variables(value, prefix + key + '_')\n","            print(f'Created variable {prefix}{key} of datatype', type(value).__name__)\n","            \n","        else:\n","            variable_name = prefix + key\n","            globals()[variable_name] = value\n","            print(f'Created variable {variable_name} of datatype', type(variable_name).__name__)       \n","\n","create_variables(config)"]},{"cell_type":"code","execution_count":null,"id":"3ac21ec0-2b66-4bf5-8a87-b34dfbed28ca","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Loaders based on the file type\n","# Loader interfaces in loaders note book\n","\n","if config_file_type == 'csv':\n","    #df = loadCSV(filePath, header, delimiter, multiLine, customSchema, escape)\n","    df = loadCSV(filePath, config_file_options_header, config_file_options_delimiter, config_file_options_multiline, config_file_options_escape)\n","\n","elif config_file_type == 'json':\n","    #df = loadJson()\n","    print('not done yet')\n","elif config_file_type == 'xml':\n","    #df = loadJson()\n","    print('not done yet')\n","elif config_file_type == 'parquet':\n","    #df = loadJson()\n","    print('not done yet')\n","else:\n","    raise Exception('ERROR: File extension not recognised, or the extension of the file is not the same as the file type defined in the config file')"]},{"cell_type":"code","execution_count":null,"id":"08a01b4b","metadata":{},"outputs":[],"source":["# Clean Columns - Removes any special chararters from the column names\n","if strToBool(config_fileOptions_clean_column_names) == True:\n","    df = clean_columns(df)\n","\n","# Check schema of loaded data frame to the setting in the config\n","if strToBool(config_fileOptions_enforce_schema) == True:\n","    dataframe_schema = df.schema\n","    custom_schema = to_dataframe_schema(config_fileOptions_custom_schema)\n","    if compare_schema(dataframe_schema, custom_schema) == False:\n","        raise Exception(\"The data frame schema does not match the config file version\")\n"," "]},{"cell_type":"code","execution_count":null,"id":"3b54e389-f631-42ab-8fec-1a7097f6b861","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Process the dataframe, and get it ready for writing to a table\n","\n","# Extend the dataframe with the columns to partition on\n","if config_table_options_raw_partition_type.lower() == 'date':\n","    df = create_date_partitions(df, config_table_options_raw_partition_date_format, config_table_options_raw_datframe_partition_columns)\n","elif rawPartitionType.lower() == 'reference':\n","    print('No partitions required')\n","# elif rawPartitionType.lower() == 'businessKeys':\n","#     df = createBusinessKeysPartitions(df, rawPartitionDateFormat, rawDataframePartitionColumns)\n","\n","# Get dataframe schema\n","\n","write_schema = df.schema\n","\n","# Does the table exist?\n","# Notes: In checking the schema we could do schema merge, however that should be handled by a version. \n","# If you are happy using schema merge into Raw remove the raise exception, \n","# and create a merge schema flag in the JSON confiig and change the write process to the table for append and merge\n","if spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == False:\n","    create_table_if_not_exists(config_table_options_raw_lakehouse_name, config_table_options_raw_layer_name\n","                               , config_table_options_raw_table_name, writeSchema, config_table_options_raw_table_partition_columns, onfig_table_options_raw_partition_type)\n","\n","elif spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == True and str_to_bool(config_table_options_raw_allow_schema_drift) == False:\n","    if df_schema_to_table(config_table_options_raw_lakehouse_name, config_table_options_raw_table_name, write_schema) == False:\n","        raise Exception('There is a schema mis-match between the dataframe and the table')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e3f7a20c-34cc-491d-b359-f708354ec5dc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if config_table_options_raw_layer_namee != \"\":\n","    adjusted_table_name = config_table_options_raw_lakehouse_name + '.' +config_table_options_raw_layer_namee + '_' + config_table_options_raw_table_name\n","else:\n","    adjusted_table_name = config_table_options_raw_table_name\n","\n","try:\n","    if str_to_bool(config_table_options_raw_structure_only) == False:\n","        if config_table_options_raw_insert_type.lower() == 'append':\n","            df.write.format(\"delta\").mode(\"append\").saveAsTable(adjustedTableName)\n","            \n","        elif config_table_options_raw_insert_type.lower() == 'overwrite':\n","            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(adjustedTableName)\n","\n","        elif config_table_options_raw_insert_type.lower() == 'merge':\n","\n","            deltaTable = DeltaTable.forName(spark, adjusted_table_name)\n","            merge_on_columns = \" AND \".join([f\"target.{col} = source.{col}\" for col in config_table_options_raw_merge_on_columns])\n","            update_columns = {f\"target.{col}\": f\"source.{col}\" for col in config_table_options_raw_merge_update_columns}\n","\n","            deltaTable.alias(\"target\").merge(\n","                source=df.alias(\"source\"), \n","                condition = merge_on_columns\n","            ).whenMatchedUpdate(\n","                set = update_columns\n","            ).whenNotMatchedInsertAll().execute()\n","    else:\n","        print('Structure only flag is set to True, no data has been loaded to the tables')\n","except:\n","    move_landing_file(file_path, 'Failure')   \n","    \n"]},{"cell_type":"code","execution_count":null,"id":"73a406c0","metadata":{},"outputs":[],"source":["move_landing_file(file_path, 'Success')"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
