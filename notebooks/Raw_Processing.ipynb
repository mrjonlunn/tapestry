{"cells":[{"cell_type":"markdown","source":["# Metadata driven Extract, Transform And Load process\n","#### Project Tapestry - https://github.com/mrjonlunn/tapestry\n","##### Created by: Jon Lunn\n","##### Version 0.3"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a1ae0a61-ac3e-461e-afee-c17399089bee"},{"cell_type":"code","source":["# Add your own parameters here, and call it from a pipeline if you like\n","file_path = 'Files/Landing/testsource/testentity/NYC1000.csv'\n","data_source = 'testSource'\n","entity = 'testEntity'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:06:40.8274167Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"a8f316fa-dea1-407d-965e-95c25ebfdd6f"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1aa4aa50-daf9-4c4c-8027-baa2fea352dc"},{"cell_type":"code","source":["%run Standard_NoteBooks"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:06:41.4067557Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"7ff1156c-1874-41f9-85bb-4a581df8d643"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"391823a5-18a1-44c6-abe9-92d2879f9730"},{"cell_type":"code","source":["# Gets the config metadata\n","config = metadata_loader(data_source, entity)\n","\n","# Function to create variables from the config file structure\n","def create_variables(config, prefix=''):\n","    # Puts config_ in front of the structure items\n","    for key, value in config.items():\n","        if isinstance(value, dict):\n","            create_variables(value, prefix + key + '_')\n","            print(f'Created variable {prefix}{key} of datatype', type(value).__name__)\n","            \n","        else:\n","            variable_name = prefix + key\n","            globals()[variable_name] = value\n","            print(f'Created variable {variable_name} of datatype', type(variable_name).__name__)     \n","\n","create_variables(config, 'config_')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:06:42.0641713Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"11804019-def3-48e5-b9c6-5e1b3e1ce57f"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Created variable config_config_version of datatype str\nCreated variable config_dataset_source of datatype str\nCreated variable config_dataset_entity of datatype str\nCreated variable config_dataset of datatype dict\nCreated variable config_file_type of datatype str\nCreated variable config_file_options_header of datatype str\nCreated variable config_file_options_delimiter of datatype str\nCreated variable config_file_options_multi_line of datatype str\nCreated variable config_file_options_escape of datatype str\nCreated variable config_file_options_clean_column_names of datatype str\nCreated variable config_file_options_enforce_schema of datatype str\nCreated variable config_file_options_custom_schema of datatype str\nCreated variable config_file_options of datatype dict\nCreated variable config_table_options_raw_lakehouse_name of datatype str\nCreated variable config_table_options_raw_layer_name of datatype str\nCreated variable config_table_options_raw_table_name of datatype str\nCreated variable config_table_options_raw_structure_only of datatype str\nCreated variable config_table_options_raw_allow_schema_drift of datatype str\nCreated variable config_table_options_raw_insert_type of datatype str\nCreated variable config_table_options_raw_create_table_if_not_exists of datatype str\nCreated variable config_table_options_raw_partition_type of datatype str\nCreated variable config_table_options_raw_partition_date_format of datatype str\nCreated variable config_table_options_raw_dataframe_partition_columns of datatype str\nCreated variable config_table_options_raw_table_partition_columns of datatype str\nCreated variable config_table_options_raw_partition_row_size of datatype str\nCreated variable config_table_options_raw_business_keys of datatype str\nCreated variable config_table_options_raw_merge_on_columns of datatype str\nCreated variable config_table_options_raw_merge_update_columns of datatype str\nCreated variable config_table_options_raw of datatype dict\nCreated variable config_table_options of datatype dict\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33227691"},{"cell_type":"code","source":["# Loaders based on the file type\n","# Loader interfaces in loaders note book\n","\n","if config_file_type == 'csv':\n","    df = loadCSV(file_path, config_file_options_header, config_file_options_delimiter, config_file_options_multi_line, config_file_options_escape)\n","\n","elif config_file_type == 'json':\n","    #df = loadJson()\n","    print('not done yet')\n","elif config_file_type == 'xml':\n","    #df = loadJson()\n","    print('not done yet')\n","elif config_file_type == 'parquet':\n","    #df = loadJson()\n","    print('not done yet')\n","else:\n","    raise Exception('ERROR: File extension not recognised, or the extension of the file is not the same as the file type defined in the config file')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:06:42.767095Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"427401cd-184a-4dab-9af0-016500663b67"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ac21ec0-2b66-4bf5-8a87-b34dfbed28ca"},{"cell_type":"code","source":["# Clean Columns - Removes any special chararters from the column names\n","if str_to_bool(config_file_options_clean_column_names) == True:\n","    df = clean_columns(df)\n","\n","# Check schema of loaded data frame to the setting in the config\n","if str_to_bool(config_file_options_enforce_schema) == True:\n","    dataframe_schema = df.schema\n","    custom_schema = to_dataframe_schema(config_file_options_custom_schema)\n","    if compare_schema(dataframe_schema, custom_schema) == False:\n","        raise Exception(\"The data frame schema does not match the config file version\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:06:44.0469635Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"04488c68-a687-4c82-b432-85d8e68a1877"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==============================\nColumns only in the read file:\n==============================\n==============================\nColumns only in the custom schema file:\n==============================\n==============================\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08a01b4b"},{"cell_type":"code","source":["# Process the dataframe, and get it ready for writing to a table\n","\n","# Extend the dataframe with the columns to partition on\n","if config_table_options_raw_partition_type.lower() == 'date':\n","    df = create_date_partitions(df, config_table_options_raw_partition_date_format, config_table_options_raw_dataframe_partition_columns)\n","elif rawPartitionType.lower() == 'reference':\n","    print('No partitions required')\n","# elif rawPartitionType.lower() == 'businessKeys':\n","#     df = createBusinessKeysPartitions(df, rawPartitionDateFormat, rawDataframePartitionColumns)\n","\n","# Get dataframe schema\n","\n","write_schema = df.schema\n","\n","# Does the table exist?\n","# Notes: In checking the schema we could do schema merge (see the option)\n","# If you are happy using schema merge into Raw remove the raise exception, \n","# and create a merge schema flag in the JSON confiig and change the write process to the table for append and merge\n","if spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == False:\n","    create_table_if_not_exists(config_table_options_raw_lakehouse_name\n","                                , config_table_options_raw_layer_name\n","                                , config_table_options_raw_table_name, write_schema\n","                                , config_table_options_raw_table_partition_columns\n","                                , config_table_options_raw_partition_type)\n","\n","elif spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == True and str_to_bool(config_table_options_raw_allow_schema_drift) == False:\n","    if df_schema_to_table(config_table_options_raw_lakehouse_name, config_table_options_raw_table_name, write_schema) == False:\n","        raise Exception('There is a schema mis-match between the dataframe and the table')\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:08:23.5020794Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"f8ffafce-92f2-4a33-abd8-43b498fe624f"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b54e389-f631-42ab-8fec-1a7097f6b861"},{"cell_type":"code","source":["if config_table_options_raw_layer_name != \"\":\n","    adjusted_table_name = config_table_options_raw_lakehouse_name + '.' +config_table_options_raw_layer_name + '_' + config_table_options_raw_table_name\n","else:\n","    adjusted_table_name = config_table_options_raw_table_name\n","\n","try:\n","    if str_to_bool(config_table_options_raw_structure_only) == False:\n","        if config_table_options_raw_insert_type.lower() == 'append':\n","            df.write.format(\"delta\").mode(\"append\").saveAsTable(adjustedTableName)\n","            \n","        elif config_table_options_raw_insert_type.lower() == 'overwrite':\n","            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(adjustedTableName)\n","\n","        elif config_table_options_raw_insert_type.lower() == 'merge':\n","\n","            deltaTable = DeltaTable.forName(spark, adjusted_table_name)\n","            merge_on_columns = \" AND \".join([f\"target.{col} = source.{col}\" for col in config_table_options_raw_merge_on_columns])\n","            update_columns = {f\"target.{col}\": f\"source.{col}\" for col in config_table_options_raw_merge_update_columns}\n","\n","            deltaTable.alias(\"target\").merge(\n","                source=df.alias(\"source\"), \n","                condition = merge_on_columns\n","            ).whenMatchedUpdate(\n","                set = update_columns\n","            ).whenNotMatchedInsertAll().execute()\n","    else:\n","        print('Structure only flag is set to True, no data has been loaded to the tables')\n","except:\n","    move_landing_file(file_path, 'Failure') "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"waiting","normalized_state":"waiting","queued_time":"2024-07-06T15:12:54.0478057Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2abce7f6-c5cd-4056-8045-a7ba31be6a0f"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3f7a20c-34cc-491d-b359-f708354ec5dc"},{"cell_type":"code","source":["move_landing_file(file_path, 'Success')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"livy_statement_state":null,"session_id":null,"state":"cancelled","normalized_state":"cancelled","queued_time":"2024-07-06T10:13:22.0618109Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-07-06T10:13:34.5070919Z","parent_msg_id":"522e29c5-64b3-4f7d-af3c-11e5540d3ad0"},"text/plain":"StatementMeta(, , , Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"73a406c0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"dependencies":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}]}}},"nbformat":4,"nbformat_minor":5}