{"cells":[{"cell_type":"markdown","source":["##### Functions\n","###### This notebook is the source for the utility functions that are called from the main processing notebook\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5e1b7422-b421-4aed-aaa7-b77c775ae9af"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7bd25cb7-b5ee-4b15-a815-083df90a10ed"},{"cell_type":"code","source":["import json\n","import yaml\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from datetime import datetime\n","from delta.tables import DeltaTable\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"23bb6cd7-f81e-4e5e-825e-2fff3874b655"},{"cell_type":"code","source":["# Assuming that the meta data files for each data source are in one folder for the data source\n","def metadata_loader(data_source, entity):\n","    # Load the folder from the standard file path\n","    config_folder = f'/lakehouse/default/Files/PipelineMetadata/{data_source}/'\n","    # Load the config file based on the source & entity\n","    cf = open(config_folder + data_source + '_' + entity + '.yaml')\n","    meta_data = yaml.safe_load(cf)\n","\n","    return(meta_data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfd9bbd4-6cad-44b4-83f0-3116a196538a"},{"cell_type":"code","source":["def template_loader(data_source, entity, config_version):\n","    \n","    template_location = f'/lakehouse/default/Files/PipelineMetadata/config_templates/{data_source}/template_{data_source}_{entity}_{config_version}.yml'\n","    tf = open(template_location)\n","    template = yaml.safe_load(tf)\n","    return template\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74af3ce7"},{"cell_type":"code","source":["def verify_config(template, config):\n","    \n","    if isinstance(template, dict) and isinstance(config, dict):\n","        # Check if the keys match\n","        if set(template.keys()) != set(config.keys()):\n","            return False\n","        # Recursively compare sub-keys\n","        for key in template.keys():\n","            if not verify_config(template[key], config[key]):\n","                return False\n","        return True\n","    elif isinstance(template, list) and isinstance(config, list):\n","        # Check if the lengths match\n","        if len(template) != len(config):\n","            return False\n","        # Recursively compare elements\n","        for i in range(len(template)):\n","            if not verify_config(template[i], config[i]):\n","                return False\n","        return True\n","    else:\n","        return True\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8dbb3558"},{"cell_type":"code","source":["def clean_columns(df):\n","    # Defines a regex pattern for special characters to be replaced\n","    pattern = re.compile(r'[.\\`\\'\",;(){}\\[\\]$*!?%&|<>+=/\\-\\\\]')\n","\n","    clean_name = lambda name: pattern.sub('_', name)\n","\n","    columns_to_clean = df.columns\n","    \n","    # Create a dictionary with old and new column names\n","    rename_dict = {col: clean_name(col) for col in columns_to_clean}\n","    \n","    # Rename columns\n","    for old_name, new_name in rename_dict.items():\n","        df = df.withColumnRenamed(old_name, new_name)\n","    \n","    return df"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"203eeacf"},{"cell_type":"code","source":["# Create schema from json file\n","def to_dataframe_schema(custom_schema):\n","\n","    fields = []\n","    for field_info in custom_schema:\n","        field_name = field_info[\"fieldName\"]\n","        field_type = field_info[\"fieldType\"]\n","\n","        if field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"IntegerType()\":\n","            field_type = IntegerType()\n","        elif field_type == \"LongType()\":\n","            field_type = LongType()\n","        elif field_type == \"FloatType()\":\n","            field_type = FloatType()\n","        elif field_type == \"DoubleType()\":\n","            field_type = DoubleType()\n","        elif field_type == \"DecimalType()\":\n","            field_type = DecimalType()\n","        elif field_type == \"StringType()\":\n","            field_type = StringType()\n","        elif field_type == \"BinaryType()\":\n","            field_type = BinaryType()\n","        elif field_type == \"BooleanType()\":\n","            field_type = BooleanType()\n","        elif field_type == \"TimestampType()\":\n","            field_type = TimestampType()\n","        elif field_type == \"DateType()\":\n","            field_type = DateType()\n","        elif field_type == \"DayTimeIntervalType()\":\n","            field_type = DayTimeIntervalType()\n","        elif field_type == \"ArrayType()\":\n","            field_type = ArrayType()\n","        elif field_type == \"MapType()\":\n","            field_type = MapType()\n","        elif field_type == \"StructField()\":\n","            field_type = StructField()\n","\n","        if isinstance(field_info[\"nullable\"], str):\n","            if field_info[\"nullable\"].lower() == 'true':\n","                nullable = True\n","            elif  field_info[\"nullable\"].lower() == 'false':\n","                nullable = False\n","            else:\n","                nullable = False\n","        elif isinstance(field_info[\"nullable\"], bool):\n","            nullable = field_info[\"nullable\"]\n","\n","        fields.append(StructField(field_name, field_type, nullable))\n","\n","    # Create the schema\n","    schema = StructType(fields)\n","    return(schema)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"23e4cb22-bc2f-40a7-99a3-b0344e4655ec"},{"cell_type":"code","source":["def move_landing_file(file_path, move_location):\n","\n","    #get and format datetime\n","    now = datetime.now()\n","    filename_timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n","    \n","    # get file name\n","    if move_location == 'Success':\n","        updated_path = file_path.replace('/Landing/', '/Processed/').replace('.', '_' + filename_timestamp + '.')\n","        mssparkutils.fs.mv(file_path, updated_path, True)\n","\n","    elif move_location == 'Failure':\n","        updated_path = file_path.replace('/Landing/', '/Failed/').replace('.', '_' + filename_timestamp + '.')\n","        mssparkutils.fs.mv(file_path, updated_path, True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"371aa491-6ddd-479d-be9c-8828e6c1b4b5"},{"cell_type":"code","source":["def create_table_if_not_exists (config_table_options_raw_lakehouse_name, config_table_options_raw_layer_name, config_table_options_raw_table_name\n","                                , write_schema, config_table_options_raw_partition_columns, config_table_options_raw_partition_type):\n","    try:\n","   \n","        if spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == False:\n","            \n","            if config_table_options_raw_layer_name != \"\":\n","                adjusted_table_name = config_table_options_raw_lakehouse_name + '.' + config_table_options_raw_layer_name + '_' + config_table_options_raw_table_name\n","            else:\n","                adjusted_table_name = config_table_options_raw_lakehouse_name + '.' +  config_table_options_raw_table_name\n","\n","            # Create a DataFrame with the specified schema (no data)\n","            data = spark.createDataFrame([], schema=write_schema)\n","            if config_table_options_raw_partition_type != 'reference':\n","                data.write.format(\"delta\").mode(\"overwrite\").partitionBy(*config_table_options_raw_partition_columns).saveAsTable(adjusted_table_name)\n","            elif config_table_options_raw_partition_type == 'reference':\n","                data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(adjusted_table_name)\n","\n","    except:\n","        print('create table failed')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3a077f98-83d0-4d35-bfc1-dd7d7c5cb023"},{"cell_type":"code","source":["def get_config_values(config, keys, valueType):\n","    current_level = config\n","    for key in keys:\n","        current_level = current_level.get(key, {})\n","        if not isinstance(current_level, dict):\n","            break\n","    \n","    return current_level if current_level else default\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b6026950-3b29-4167-a25e-f84bf074a5fc"},{"cell_type":"code","source":["# Added to remove the issue of mix of string and bool values you can get in the config\n","def str_to_bool(string):\n","     if isinstance(string, bool):\n","          return(string)\n","     else:\n","          if string.lower() == 'true':\n","               return True\n","          elif string.lower() == 'false':\n","               return False"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fa8938e-d89b-48b7-858e-af78c020f79a"},{"cell_type":"code","source":["def df_schema_to_table(config_tableOptions_raw_lakehouse_name, config_tableOptions_raw_table_name, df_schema):\n","    # Note this just check column names and not order\n","    # for ordering use tableSchema.fieldNames() == dfSchema.fieldNames()\n","    table = spark.table(config_tableOptions_raw_lakehouse_name + '.' + config_tableOptions_raw_table_name)\n","    table_schema = table.schema\n","\n","    if table_schema == df_schema:\n","        return(True)\n","    else:\n","        return(False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"396a7a99-7047-49cc-b3ad-85fd435f2476"},{"cell_type":"code","source":["def create_date_partitions(df, config_tableOptions_raw_partition_date_format, onfig_tableOptions_raw_dataframe_partition_columns):\n","\n","    date_columns = config_tableOptions_raw_partition_date_format.split('/')\n","    date_columns = [x.lower() for x in date_columns]\n","\n","    # date_format(\"date\", \"yyyyMM\")). \\\n","    if 'yyyy' in date_columns:\n","        df = df.withColumn(\"PartitionYear\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"yyyy\"))\n","    if 'mm' in date_columns:\n","        df = df.withColumn(\"PartitionMonth\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"MM\"))\n","    if 'dd' in date_columns:\n","        df = df.withColumn(\"PartitionDay\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"dd\"))\n","\n","    return(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a527f53-a993-4100-a808-9b9a6576c6fa"},{"cell_type":"code","source":["def compare_schema(dataframe_schema, custom_schema):\n","    # Leave out added metadata as they have been added after load\n","    columns_to_exclude = ['MetaCreatedDate', 'MetaUpdatedDate', 'MetaSourceFilename']\n","    \n","    # Remove meta columns\n","    dataframe_schema = StructType([field for field in dataframe_schema if field.name not in columns_to_exclude])\n","\n","\n","    df_fields = set((field.name, str(field.dataType)) for field in dataframe_schema)\n","    df_custom = set((field.name, str(field.dataType)) for field in custom_schema)\n","    only_in_df_fields = df_fields - df_custom\n","    only_in_df_custom = df_custom - df_fields\n","    \n","    print('==============================')\n","    print('Columns only in the read file:')\n","    print('==============================')\n","    for field in only_in_df_fields:\n","        print(f'Column: {field[0]}, Type: {field[1]}')\n","    \n","    print('==============================')\n","    print('Columns only in the custom schema file:')\n","    print('==============================')\n","    for field in only_in_df_custom:\n","        print(f'Column: {field[0]}, Type: {field[1]}')\n","    print('==============================')\n","\n","    return dataframe_schema == custom_schema\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2ccc6292"},{"cell_type":"code","source":["def get_lakehouse_Id(config_tableOptions_raw_lakehouse_name):\n","    lakehouses = spark.catalog.listDatabases()\n","    for lakehouse in lakehouses:\n","        if lakehouse.name == config_tableOptions_raw_lakehouse_name:\n","            uri_parts = lakehouse.locationUri.split('/')\n","            lakehouse_Id = uri_parts[-2]\n","    return(lakehouse_Id)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be6acdf3-df60-4a96-8b42-6d351b3421e7"},{"cell_type":"code","source":["# Basic rasie error, not yet implimented\n","def basic_error(message):\n","    raise Exception(message)\n","    mssparkutils.notebook.exit(message)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a7b6ff2e-7eb9-4c98-90e5-4300c7f0c6c7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"dependencies":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}]}}},"nbformat":4,"nbformat_minor":5}