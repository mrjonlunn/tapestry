{"cells":[{"cell_type":"markdown","id":"5e1b7422-b421-4aed-aaa7-b77c775ae9af","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["##### Functions\n","###### This notebook is the source for the utility functions that are called from the main processing notebook\n","\n"]},{"cell_type":"code","execution_count":null,"id":"7bd25cb7-b5ee-4b15-a815-083df90a10ed","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"]},{"cell_type":"code","execution_count":null,"id":"23bb6cd7-f81e-4e5e-825e-2fff3874b655","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import json\n","import yaml\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from datetime import datetime\n","from delta.tables import DeltaTable\n"]},{"cell_type":"code","execution_count":null,"id":"cfd9bbd4-6cad-44b4-83f0-3116a196538a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Assuming that the meta data files for each data source are in one folder for the data source\n","def metadata_loader(data_source, entity):\n","    # Load the folder from the standard file path\n","    config_folder = f'/lakehouse/default/Files/PipelineMetadata/{dataSource}/'\n","    # Load the config file based on the source & entity\n","    cf = open(config_folder + data_source + '_' + entity + '.yml')\n","    meta_data = yaml.safe_load(cf)\n","\n","    return(meta_data)"]},{"cell_type":"code","execution_count":null,"id":"74af3ce7","metadata":{},"outputs":[],"source":["def template_loader(data_source, entity, config_version):\n","    \n","    template_location = f'/lakehouse/default/Files/PipelineMetadata/config_templates/{data_source}/template_{data_source}_{entity}_{config_version}.yml'\n","    tf = open(template_location)\n","    template = yaml.safe_load(tf)\n","    return template\n"]},{"cell_type":"code","execution_count":null,"id":"8dbb3558","metadata":{},"outputs":[],"source":["def verify_config(template, config):\n","    \n","    if isinstance(template, dict) and isinstance(config, dict):\n","        # Check if the keys match\n","        if set(template.keys()) != set(config.keys()):\n","            return False\n","        # Recursively compare sub-keys\n","        for key in template.keys():\n","            if not verify_config(template[key], config[key]):\n","                return False\n","        return True\n","    elif isinstance(template, list) and isinstance(config, list):\n","        # Check if the lengths match\n","        if len(template) != len(config):\n","            return False\n","        # Recursively compare elements\n","        for i in range(len(template)):\n","            if not verify_config(template[i], config[i]):\n","                return False\n","        return True\n","    else:\n","        return True\n"]},{"cell_type":"code","execution_count":null,"id":"203eeacf","metadata":{},"outputs":[],"source":["def clean_columns(df):\n","    # Defines a regex pattern for special characters to be replaced\n","    pattern = re.compile(r'[.\\`\\'\",;(){}\\[\\]$*!?%&|<>+=/\\-\\\\]')\n","\n","    clean_name = lambda name: pattern.sub('_', name)\n","\n","    columns_to_clean = df.columns\n","    \n","    # Create a dictionary with old and new column names\n","    rename_dict = {col: clean_name(col) for col in columns_to_clean}\n","    \n","    # Rename columns\n","    for old_name, new_name in rename_dict.items():\n","        df = df.withColumnRenamed(old_name, new_name)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"id":"23e4cb22-bc2f-40a7-99a3-b0344e4655ec","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Create schema from json file\n","def to_dataframe_schema(custom_schema):\n","\n","    fields = []\n","    for field_info in custom_schema:\n","        field_name = field_info[\"fieldName\"]\n","        field_type = field_info[\"fieldType\"]\n","\n","        if field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"IntegerType()\":\n","            field_type = IntegerType()\n","        elif field_type == \"LongType()\":\n","            field_type = LongType()\n","        elif field_type == \"FloatType()\":\n","            field_type = FloatType()\n","        elif field_type == \"DoubleType()\":\n","            field_type = DoubleType()\n","        elif field_type == \"DecimalType()\":\n","            field_type = DecimalType()\n","        elif field_type == \"StringType()\":\n","            field_type = StringType()\n","        elif field_type == \"BinaryType()\":\n","            field_type = BinaryType()\n","        elif field_type == \"BooleanType()\":\n","            field_type = BooleanType()\n","        elif field_type == \"TimestampType()\":\n","            field_type = TimestampType()\n","        elif field_type == \"DateType()\":\n","            field_type = DateType()\n","        elif field_type == \"DayTimeIntervalType()\":\n","            field_type = DayTimeIntervalType()\n","        elif field_type == \"ArrayType()\":\n","            field_type = ArrayType()\n","        elif field_type == \"MapType()\":\n","            field_type = MapType()\n","        elif field_type == \"StructField()\":\n","            field_type = StructField()\n","\n","\n","        if field_info[\"nullable\"].lower() == 'true':\n","            nullable = True\n","        elif  field_info[\"nullable\"].lower() == 'false':\n","            nullable = False\n","        else:\n","            nullable = False\n","\n","        fields.append(StructField(field_name, field_type, nullable))\n","\n","    # Create the schema\n","    schema = StructType(fields)\n","    return(schema)"]},{"cell_type":"code","execution_count":null,"id":"371aa491-6ddd-479d-be9c-8828e6c1b4b5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def move_landing_file(file_path, move_location):\n","\n","    #get and format datetime\n","    now = datetime.now()\n","    filename_timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n","    \n","    # get file name\n","    if move_location == 'Success':\n","        updated_path = file_path.replace('/Landing/', '/Processed/').replace('.', '_' + filename_timestamp + '.')\n","        mssparkutils.fs.mv(file_path, updated_path, True)\n","\n","    elif move_location == 'Failure':\n","        updated_path = file_path.replace('/Landing/', '/Failed/').replace('.', '_' + filename_timestamp + '.')\n","        mssparkutils.fs.mv(file_path, updated_path, True)"]},{"cell_type":"code","execution_count":null,"id":"3a077f98-83d0-4d35-bfc1-dd7d7c5cb023","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def create_table_if_not_exists (config_table_options_raw_lakehouse_name, config_table_options_raw_layer_name, config_table_options_raw_table_name\n","                                , write_schema, config_table_options_raw_partition_columns, config_table_options_raw_partition_type):\n","    try:\n","   \n","        if spark.catalog.tableExists(config_table_options_raw_table_name, config_table_options_raw_lakehouse_name) == False:\n","            \n","            if config_table_options_raw_layer_name != \"\":\n","                adjusted_table_name = config_table_options_raw_lakehouse_name + '.' + config_table_options_raw_layer_name + '_' + config_table_options_raw_table_name\n","            else:\n","                adjusted_table_name = config_table_options_raw_lakehouse_name + '.' +  config_table_options_raw_table_name\n","\n","            # Create a DataFrame with the specified schema (no data)\n","            data = spark.createDataFrame([], schema=write_schema)\n","            if config_table_options_raw_partition_type != 'reference':\n","                data.write.format(\"delta\").mode(\"overwrite\").partitionBy(*config_table_options_raw_partition_columns).saveAsTable(adjusted_table_name)\n","            elif config_table_options_raw_partition_type == 'reference':\n","                data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(adjusted_table_name)\n","\n","    except:\n","        print('create table failed')\n"]},{"cell_type":"code","execution_count":null,"id":"b6026950-3b29-4167-a25e-f84bf074a5fc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def get_config_values(config, keys, valueType):\n","    current_level = config\n","    for key in keys:\n","        current_level = current_level.get(key, {})\n","        if not isinstance(current_level, dict):\n","            break\n","    \n","    return current_level if current_level else default\n"]},{"cell_type":"code","execution_count":null,"id":"1fa8938e-d89b-48b7-858e-af78c020f79a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Added to remove the issue of mix of string and bool values you can get in the config\n","def str_to_bool(string):\n","     if isinstance(string, bool):\n","          return(string)\n","     else:\n","          if string.lower() == 'true':\n","               return True\n","          elif string.lower() == 'false':\n","               return False"]},{"cell_type":"code","execution_count":null,"id":"396a7a99-7047-49cc-b3ad-85fd435f2476","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def df_schema_to_table(config_tableOptions_raw_lakehouse_name, config_tableOptions_raw_table_name, df_schema):\n","    # Note this just check column names and not order\n","    # for ordering use tableSchema.fieldNames() == dfSchema.fieldNames()\n","    table = spark.table(config_tableOptions_raw_lakehouse_name + '.' + config_tableOptions_raw_table_name)\n","    table_schema = table.schema\n","\n","    if table_schema == df_schema:\n","        return(True)\n","    else:\n","        return(False)"]},{"cell_type":"code","execution_count":null,"id":"9a527f53-a993-4100-a808-9b9a6576c6fa","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def create_date_partitions(df, config_tableOptions_raw_partition_date_format, onfig_tableOptions_raw_dataframe_partition_columns):\n","\n","    date_columns = config_tableOptions_raw_partition_date_format.split('/')\n","    date_columns = [x.lower() for x in date_columns]\n","\n","    # date_format(\"date\", \"yyyyMM\")). \\\n","    if 'yyyy' in date_columns:\n","        df = df.withColumn(\"PartitionYear\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"yyyy\"))\n","    if 'mm' in date_columns:\n","        df = df.withColumn(\"PartitionMonth\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"MM\"))\n","    if 'dd' in date_columns:\n","        df = df.withColumn(\"PartitionDay\", date_format(onfig_tableOptions_raw_dataframe_partition_columns, \"dd\"))\n","\n","    return(df)"]},{"cell_type":"code","execution_count":null,"id":"2ccc6292","metadata":{},"outputs":[],"source":["def compare_schema(dataframe_schema, custom_schema):\n","    # Leave out added metadata as they have been added after load\n","    columns_to_exclude = ['MetaCreatedDate', 'MetaUpdatedDate', 'MetaSourceFilename']\n","    \n","    # Remove meta columns\n","    dataframe_schema = StructType([field for field in dataframe_schema if field.name not in columns_to_exclude])\n","\n","    return dataframe_schema == custom_schema\n"]},{"cell_type":"code","execution_count":null,"id":"be6acdf3-df60-4a96-8b42-6d351b3421e7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def get_lakehouse_Id(config_tableOptions_raw_lakehouse_name):\n","    lakehouses = spark.catalog.listDatabases()\n","    for lakehouse in lakehouses:\n","        if lakehouse.name == config_tableOptions_raw_lakehouse_name:\n","            uri_parts = lakehouse.locationUri.split('/')\n","            lakehouse_Id = uri_parts[-2]\n","    return(lakehouse_Id)"]},{"cell_type":"code","execution_count":null,"id":"a7b6ff2e-7eb9-4c98-90e5-4300c7f0c6c7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Basic rasie error, not yet implimented\n","def basic_error(message):\n","    raise Exception(message)\n","    mssparkutils.notebook.exit(message)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
