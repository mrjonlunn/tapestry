{"cells":[{"cell_type":"markdown","source":["##### Functions\n","###### This notebook is the source for the utility functions that are called from the main processing notebook\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5e1b7422-b421-4aed-aaa7-b77c775ae9af"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7bd25cb7-b5ee-4b15-a815-083df90a10ed"},{"cell_type":"code","source":["import json\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from datetime import datetime\n","from delta.tables import DeltaTable\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"23bb6cd7-f81e-4e5e-825e-2fff3874b655"},{"cell_type":"code","source":["def metadataLoader(filePath, configFolder):\n","    # Strip out the names from the folder location\n","    f = filePath.split('/')\n","    l = len(f)\n","    sourceFolder = f[2]\n","    entityFolder = f[3]\n","    # Load the config file based on the source & entity\n","    cf = open(configFolder + sourceFolder + '_' + entityFolder + '.json')\n","    mdata = json.load(cf)\n","    return(mdata)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cfd9bbd4-6cad-44b4-83f0-3116a196538a"},{"cell_type":"code","source":["# Create schema from json file\n","def jsonToSchema(custom_schema):\n","\n","    fields = []\n","    for field_info in custom_schema:\n","        field_name = field_info[\"fieldName\"]\n","        field_type = field_info[\"fieldType\"]\n","\n","        if field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"ByteType()\":\n","            field_type = ByteType()\n","        elif field_type == \"IntegerType()\":\n","            field_type = IntegerType()\n","        elif field_type == \"LongType()\":\n","            field_type = LongType()\n","        elif field_type == \"FloatType()\":\n","            field_type = FloatType()\n","        elif field_type == \"DoubleType()\":\n","            field_type = DoubleType()\n","        elif field_type == \"DecimalType()\":\n","            field_type = DecimalType()\n","        elif field_type == \"StringType()\":\n","            field_type = StringType()\n","        elif field_type == \"BinaryType()\":\n","            field_type = BinaryType()\n","        elif field_type == \"BooleanType()\":\n","            field_type = BooleanType()\n","        elif field_type == \"TimestampType()\":\n","            field_type = TimestampType()\n","        elif field_type == \"DateType()\":\n","            field_type = DateType()\n","        elif field_type == \"DayTimeIntervalType()\":\n","            field_type = DayTimeIntervalType()\n","        elif field_type == \"ArrayType()\":\n","            field_type = ArrayType()\n","        elif field_type == \"MapType()\":\n","            field_type = MapType()\n","        elif field_type == \"StructField()\":\n","            field_type = StructField()\n","\n","\n","        if field_info[\"nullable\"].lower() == 'true':\n","            nullable = True\n","        elif  field_info[\"nullable\"].lower() == 'false':\n","            nullable = False\n","        else:\n","            nullable = False\n","\n","        fields.append(StructField(field_name, field_type, nullable))\n","\n","    # Create the schema\n","    schema = StructType(fields)\n","    return(schema)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"23e4cb22-bc2f-40a7-99a3-b0344e4655ec"},{"cell_type":"code","source":["def moveLandingFile(filePath, moveLocation):\n","\n","    #get and format datetime\n","    now = datetime.now()\n","    filenameTimestamp = now.strftime(\"%Y%m%d%H%M%S\")\n","    \n","    # get file name\n","    if moveLocation == 'Success':\n","        updatedPath = filePath.replace('/Landing/', '/Processed/').replace('.', '_' + filenameTimestamp + '.')\n","        mssparkutils.fs.mv(filePath, updatedPath, True)\n","\n","    elif moveLocation == 'Failure':\n","        updatedPath = filePath.replace('/Landing/', '/Failed/').replace('.', '_' + filenameTimestamp + '.')\n","        mssparkutils.fs.mv(filePath, updatedPath, True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"371aa491-6ddd-479d-be9c-8828e6c1b4b5"},{"cell_type":"code","source":["def createTableIfNotExists (lakehouseName, layerName, tableName, writeSchema, partitionColumns, rawPartitionType):\n","   \n","    if spark.catalog.tableExists(tableName, lakehouseName) == False and len(partitionColumns) != 0:\n","        if rawLayerName != \"\":\n","            adjustedTableName = lakehouseName + '.' + rawLayerName + '_' + rawTableName\n","        else:\n","            adjustedTableName = lakehouseName + '.' +  rawTableName\n","\n","        # Create a DataFrame with the specified schema (no data)\n","        data = spark.createDataFrame([], schema=writeSchema)\n","        data.write.format(\"delta\").mode(\"overwrite\").partitionBy(*rawTablePartitionColumns).saveAsTable(adjustedTableName)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3a077f98-83d0-4d35-bfc1-dd7d7c5cb023"},{"cell_type":"code","source":["def getConfigValues(config, keys, valueType):\n","    current_level = config\n","    for key in keys:\n","        current_level = current_level.get(key, {})\n","        if not isinstance(current_level, dict):\n","            break\n","    \n","    \n","    return current_level if current_level else default\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b6026950-3b29-4167-a25e-f84bf074a5fc"},{"cell_type":"code","source":["def strToBool(string):\n","    if string.lower() == 'true':\n","         return True\n","    elif string.lower() == 'false':\n","         return False"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1fa8938e-d89b-48b7-858e-af78c020f79a"},{"cell_type":"code","source":["def schemaDFToTable(lakehouseName, tableName, dfSchema):\n","    # Note this just check column names and not order\n","    # for ordering use tableSchema.fieldNames() == dfSchema.fieldNames()\n","    table = spark.table(lakehouseName + '.' + tableName)\n","    tableSchema = table.schema\n","\n","    if tableSchema == dfSchema:\n","        return(True)\n","    else:\n","        return(False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"396a7a99-7047-49cc-b3ad-85fd435f2476"},{"cell_type":"code","source":["def createDatePartitions(df, partitionDateFormat, rawDataframePartitionColumns):\n","\n","    dateColumns = partitionDateFormat.split('/')\n","    dateColumns = [x.lower() for x in dateColumns]\n","\n","    # date_format(\"date\", \"yyyyMM\")). \\\n","    if 'yyyy' in dateColumns:\n","        df = df.withColumn(\"PartitionYear\", date_format(rawDataframePartitionColumns, \"yyyy\"))\n","    if 'mm' in dateColumns:\n","        df = df.withColumn(\"PartitionMonth\", date_format(rawDataframePartitionColumns, \"MM\"))\n","    if 'dd' in dateColumns:\n","        df = df.withColumn(\"PartitionDay\", date_format(rawDataframePartitionColumns, \"dd\"))\n","\n","    return(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9a527f53-a993-4100-a808-9b9a6576c6fa"},{"cell_type":"code","source":["def getLakehouseId(lakehouseName):\n","    lakehouses = spark.catalog.listDatabases()\n","    for lakehouse in lakehouses:\n","        if lakehouse.name == lakehouseName:\n","            uri_parts = lakehouse.locationUri.split('/')\n","            lakehouseId = uri_parts[-2]\n","    return(lakehouseId)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"be6acdf3-df60-4a96-8b42-6d351b3421e7"},{"cell_type":"code","source":["# Basic rasie error, not yet implimented\n","def basicError(message):\n","    raise Exception(message)\n","    mssparkutils.notebook.exit(message)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a7b6ff2e-7eb9-4c98-90e5-4300c7f0c6c7"}],"metadata":{"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{"default_lakehouse":"7a1b884b-6164-410b-b484-03e6c20c2abe","known_lakehouses":[{"id":"7a1b884b-6164-410b-b484-03e6c20c2abe"}],"default_lakehouse_name":"LH_FabricPoC","default_lakehouse_workspace_id":"eee3c34c-f4d2-48d4-bbd2-0101a39d1896"}}},"nbformat":4,"nbformat_minor":5}